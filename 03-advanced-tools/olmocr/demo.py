#!/usr/bin/env python3
"""
olmOCR PDFæ•¸æ“šè™•ç†å¯¦ç¾
"""

import subprocess
import sys
import os
import time
import json
from pathlib import Path

# é…ç½®ï¼šæŒ‡å®šä½¿ç”¨çš„ GPU è¨­å‚™ï¼ˆNone è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ GPUï¼Œ0 è¡¨ç¤º GPU0ï¼Œ1 è¡¨ç¤º GPU1ï¼‰
GPU_DEVICE = 1  # è¨­ç½®ç‚º None ä½¿ç”¨æ‰€æœ‰ GPUï¼Œæˆ–è¨­ç½®ç‚º 0, 1, 2... æŒ‡å®šç‰¹å®š GPU

def check_gpu_memory():
    """æª¢æŸ¥GPUè¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"ç™¼ç¾ {gpu_count} å€‹ GPU:")
            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                memory_total = props.total_memory / (1024**3)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)
                reserved = torch.cuda.memory_reserved(i) / (1024**3)
                memory_free = memory_total - reserved
                marker = "ğŸ‘‰" if GPU_DEVICE == i else "  "
                print(f"{marker} GPU {i}: {allocated:.1f}GB å·²åˆ†é… / {reserved:.1f}GB å·²ä¿ç•™ / {memory_free:.1f}GB å¯ç”¨ / {memory_total:.1f}GB ç¸½è¨ˆ")
            
            if GPU_DEVICE is not None:
                if GPU_DEVICE >= gpu_count:
                    print(f"âš ï¸  è­¦å‘Šï¼šæŒ‡å®šçš„ GPU {GPU_DEVICE} ä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨æ‰€æœ‰ GPU")
                else:
                    print(f"âœ… å°‡ä½¿ç”¨ GPU {GPU_DEVICE}")
            else:
                print(f"âœ… å°‡ä½¿ç”¨æ‰€æœ‰ GPU")
            return True
        else:
            print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")
            return False
    except Exception as e:
        print(f"âš ï¸  GPUæª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_olmocr():
    """æª¢æŸ¥ olmOCR æ˜¯å¦å·²å®‰è£"""
    try:
        import olmocr
        print(f"âœ… olmOCR å·²å®‰è£")
        return True
    except ImportError:
        print("âŒ olmOCR æœªå®‰è£")
        print("   è«‹åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£ olmOCR:")
        print("   uv pip install 'olmocr[gpu]' --extra-index-url https://download.pytorch.org/whl/cu128")
        print("   è©³ç´°èªªæ˜è«‹åƒè€ƒ README.md")
        return False
    except Exception as e:
        print(f"âš ï¸  olmOCR æª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_vllm():
    """æª¢æŸ¥ vLLM æ˜¯å¦å·²å®‰è£ï¼ˆolmOCR çš„æ¨ç†å¼•æ“ï¼‰"""
    try:
        import vllm
        import torch
        print(f"âœ… vLLM å·²å®‰è£ (ç‰ˆæœ¬: {vllm.__version__})")
        print(f"âœ… PyTorch å·²å®‰è£ (ç‰ˆæœ¬: {torch.__version__})")

        # æª¢æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
        torch_version = torch.__version__
        vllm_version = vllm.__version__

        # æ ¹æ“šå®˜ç¶²æ–‡æª”ï¼ŒPyTorch 2.7+ èˆ‡ vLLM 0.11.0+ å®Œå…¨å…¼å®¹
        print(f"âœ… PyTorch {torch_version} èˆ‡ vLLM {vllm_version} å…¼å®¹")

        return True
    except ImportError:
        print("âš ï¸  vLLM æœªå®‰è£ï¼ˆolmOCR çš„æ¨ç†å¼•æ“ï¼‰")
        print("   é€™é€šå¸¸æœƒåœ¨å®‰è£ olmocr[gpu] æ™‚è‡ªå‹•å®‰è£")
        print("   å¦‚æœé‡åˆ°å•é¡Œï¼Œè«‹åƒè€ƒ README.md")
        return False
    except Exception as e:
        print(f"âš ï¸  vLLM æª¢æŸ¥å¤±æ•—: {e}")
        return False

def pre_download_model():
    """é ä¸‹è¼‰æ¨¡å‹ä»¥é¿å…è½‰æ›æ™‚timeout"""
    print("ğŸ”½ é å…ˆä¸‹è¼‰OLMoCRæ¨¡å‹...")

    try:
        from huggingface_hub import snapshot_download
        model_id = "allenai/olmOCR-2-7B-1025-FP8"

        print(f"ä¸‹è¼‰æ¨¡å‹: {model_id}")
        cache_dir = snapshot_download(
            repo_id=model_id,
            local_files_only=False,  # å…è¨±ä¸‹è¼‰
            resume_download=True     # æ”¯æ´æ–·é»çºŒå‚³
        )
        print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆï¼Œå¿«å–ä½ç½®: {cache_dir}")
        return True

    except ImportError:
        print("âŒ huggingface_hubæœªå®‰è£ï¼Œè·³éé ä¸‹è¼‰")
        return False
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
        return False

def process_pdfs():
    """è™•ç†test_pdfsç›®éŒ„ä¸‹çš„PDFæ–‡ä»¶"""
    # æŒ‡å‘çˆ¶ç›®éŒ„çš„ test_pdfs
    test_dir = Path(__file__).parent.parent / "test_pdfs"
    if not test_dir.exists():
        print("âŒ test_pdfsç›®éŒ„ä¸å­˜åœ¨")
        print(f"   é æœŸè·¯å¾‘: {test_dir.absolute()}")
        return []

    pdf_files = list(test_dir.glob("*.pdf"))
    if not pdf_files:
        print("âŒ ç„¡PDFæ–‡ä»¶")
        return []

    print(f"ğŸ“ ç™¼ç¾ {len(pdf_files)} å€‹PDF")
    results = []

    for pdf in pdf_files:
        size_mb = pdf.stat().st_size / (1024*1024)
        print(f"è™•ç†: {pdf.name} ({size_mb:.1f}MB)")

        start_time = time.time()
        result = convert_pdf(pdf)
        process_time = time.time() - start_time

        result_data = {
            'file': pdf.name,
            'size_mb': size_mb,
            'process_time': process_time,
            'success': result['success'],
            'output_size': result.get('output_size', 0),
            'error': result.get('error', None)
        }
        
        # æ·»åŠ æˆåŠŸæ™‚çš„é¡å¤–ä¿¡æ¯
        if result['success']:
            result_data['md_count'] = result.get('md_count', 0)
            result_data['json_count'] = result.get('json_count', 0)
            result_data['output_dir'] = result.get('output_dir', '')
            result_data['warning'] = result.get('warning', None)
            
            if result_data['md_count'] > 0:
                print(f"  âœ… æˆåŠŸï¼ç”Ÿæˆ {result_data['md_count']} å€‹ Markdown æ–‡ä»¶")
                if result.get('files'):
                    print(f"     æ–‡ä»¶ä½ç½®: {', '.join(result.get('files', [])[:2])}")
                    if len(result.get('files', [])) > 2:
                        print(f"     ... é‚„æœ‰ {len(result.get('files', [])) - 2} å€‹æ–‡ä»¶")
            elif result_data['json_count'] > 0:
                print(f"  âœ… æˆåŠŸï¼ç”Ÿæˆ {result_data['json_count']} å€‹ JSON æ–‡ä»¶")
            else:
                warning = result_data.get('warning', '')
                if warning:
                    print(f"  âš ï¸  {warning}")
                else:
                    print(f"  âš ï¸  è™•ç†å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¼¸å‡ºæ–‡ä»¶")
        else:
            error_msg = result.get('error', 'æœªçŸ¥éŒ¯èª¤')
            # æˆªæ–·éé•·çš„éŒ¯èª¤ä¿¡æ¯
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."
            print(f"  âŒ å¤±æ•—: {error_msg}")
        
        results.append(result_data)

    return results

def convert_pdf(pdf_path):
    """ä½¿ç”¨olmOCRè½‰æ›PDF"""
    try:
        # å‰µå»ºè¼¸å‡ºç›®éŒ„ï¼ˆworkspaceï¼‰
        output_dir = Path(__file__).parent / "output"
        output_dir.mkdir(exist_ok=True)
        
        # olmOCR ä½¿ç”¨ workspace ç›®éŒ„ï¼Œæ‰€æœ‰æ–‡ä»¶æœƒæ”¾åœ¨é€™è£¡
        workspace_dir = output_dir / "workspace"
        workspace_dir.mkdir(exist_ok=True)

        # æ­£ç¢ºçš„å‘½ä»¤æ ¼å¼ï¼špython -m olmocr.pipeline <workspace> --pdfs <pdf_file> --markdown
        # ä½¿ç”¨ç•¶å‰ Python è§£é‡‹å™¨
        python_executable = sys.executable
        
        # åŸºæœ¬å‘½ä»¤ï¼ˆä½¿ç”¨é©ä¸­çš„è¨˜æ†¶é«”è¨­ç½®ä»¥ç¢ºä¿ç©©å®šé‹è¡Œï¼‰
        cmd = [
            python_executable, "-m", "olmocr.pipeline",
            str(workspace_dir),  # workspace ä½ç½®åƒæ•¸
            "--pdfs", str(pdf_path),  # PDF æ–‡ä»¶
            "--max_page_error_rate", "0.1",  # å…è¨±10%çš„é é¢éŒ¯èª¤ç‡ï¼ˆå¿…éœ€åƒæ•¸ï¼‰
        ]
        
        print(f"  åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"  â±ï¸  è¨­å®šè¶…æ™‚: 900ç§’")
        print(f"  ğŸ“„ å…è¨±é é¢éŒ¯èª¤ç‡: 10%ï¼ˆä½¿ç”¨é è¨­è¨­å®šï¼‰")
        if GPU_DEVICE is not None:
            print(f"  ğŸ¯ ä½¿ç”¨ GPU {GPU_DEVICE}")
        
        # è¨­ç½® CUDA_VISIBLE_DEVICES ç’°å¢ƒè®Šé‡ä¾†æŒ‡å®š GPU
        env = os.environ.copy()
        if GPU_DEVICE is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(GPU_DEVICE)
            print(f"  ğŸ”§ ç’°å¢ƒè®Šé‡ CUDA_VISIBLE_DEVICES={GPU_DEVICE}")
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=900, env=env)

        # æª¢æŸ¥ stderr ä¸­æ˜¯å¦æœ‰éŒ¯èª¤ï¼ˆå³ä½¿è¿”å›ç¢¼ç‚º 0ï¼Œä¹Ÿå¯èƒ½æœ‰éŒ¯èª¤ï¼‰
        error_output = result.stderr.strip() or result.stdout.strip()
        # æª¢æŸ¥å„ç¨®éŒ¯èª¤é¡å‹
        has_gpu_memory_error = error_output and ('gpu memory' in error_output.lower() or 'kv cache is larger' in error_output.lower() or 'out of memory' in error_output.lower())
        has_compatibility_error = error_output and ('attributeerror' in error_output.lower() and '_inductor' in error_output.lower() and 'config' in error_output.lower())
        has_vllm_server_error = error_output and ('vllm server task ended' in error_output.lower() or 'vllm server' in error_output.lower())
        has_error = error_output and ('error' in error_output.lower() or 'not found' in error_output.lower() or 'traceback' in error_output.lower() or has_gpu_memory_error or has_compatibility_error or has_vllm_server_error)
        
        # æª¢æŸ¥è¿”å›ç¢¼å’Œè¼¸å‡º
        if result.returncode == 0 and not has_error:
            # olmOCR æœƒåœ¨ workspace ç›®éŒ„ä¸­å‰µå»ºæ–‡ä»¶
            # ä½¿ç”¨ --markdown æ™‚ï¼Œæ–‡ä»¶æœƒåœ¨ markdown/ å­ç›®éŒ„ä¸­
            # æŸ¥æ‰¾ç”Ÿæˆçš„ markdown æ–‡ä»¶
            md_files = list(workspace_dir.rglob('*.md'))
            if md_files:
                output_size = sum(f.stat().st_size for f in md_files)
                # é¡¯ç¤ºç”Ÿæˆçš„æ–‡ä»¶è·¯å¾‘ï¼ˆæœ€å¤šé¡¯ç¤ºå‰3å€‹ï¼‰
                file_preview = [str(f.relative_to(workspace_dir)) for f in md_files[:3]]
                return {
                    'success': True, 
                    'output_size': output_size, 
                    'md_count': len(md_files), 
                    'output_dir': str(workspace_dir),
                    'files': file_preview
                }
            else:
                # ä¹Ÿæª¢æŸ¥å…¶ä»–å¯èƒ½çš„è¼¸å‡ºæ ¼å¼ï¼ˆDolma JSON ç­‰ï¼‰
                json_files = list(workspace_dir.rglob('*.json'))
                if json_files:
                    output_size = sum(f.stat().st_size for f in json_files)
                    return {'success': True, 'output_size': output_size, 'json_count': len(json_files), 'output_dir': str(workspace_dir)}
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ–‡ä»¶ç”Ÿæˆ
                all_files = list(workspace_dir.rglob('*'))
                if all_files:
                    # æœ‰æ–‡ä»¶ä½†æ ¼å¼ä¸å°ï¼Œè¿”å›ä¿¡æ¯
                    file_types = set(f.suffix for f in all_files if f.is_file())
                    return {
                        'success': True, 
                        'output_size': 0, 
                        'md_count': 0, 
                        'output_dir': str(workspace_dir),
                        'warning': f'ç”Ÿæˆäº†æ–‡ä»¶ä½†æœªæ‰¾åˆ° .md æˆ– .json æ ¼å¼ï¼Œç™¼ç¾çš„æ–‡ä»¶é¡å‹: {file_types}'
                    }
                
                # æ²’æœ‰ç”Ÿæˆä»»ä½•æ–‡ä»¶ï¼Œæª¢æŸ¥éŒ¯èª¤è¼¸å‡º
                if error_output:
                    # æª¢æŸ¥å„ç¨®éŒ¯èª¤é¡å‹
                    if has_compatibility_error:
                        error_msg = "vLLM å…§éƒ¨éŒ¯èª¤ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¼‰å…¥æˆ–é…ç½®å•é¡Œ"
                        error_lines = [line for line in error_output.split('\n') if 'attributeerror' in line.lower() and '_inductor' in line.lower()]
                        if error_lines:
                            error_msg += f" (è©³ç´°: {error_lines[0][:200]})"
                    elif has_gpu_memory_error:
                        error_msg = "GPU è¨˜æ†¶é«”ä¸è¶³ã€‚å»ºè­°é™ä½ --gpu-memory-utilization æˆ– --max_model_len åƒæ•¸"
                        # æå–è©³ç´°éŒ¯èª¤ä¿¡æ¯
                        error_lines = [line for line in error_output.split('\n') if 'gpu memory' in line.lower() or 'kv cache' in line.lower() or 'memory' in line.lower()]
                        if error_lines:
                            error_msg = error_lines[0][:300]
                    elif has_vllm_server_error:
                        error_msg = "vLLM æœå‹™å™¨å•Ÿå‹•å¤±æ•—ã€‚å¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ"
                        error_lines = [line for line in error_output.split('\n') if 'vllm server' in line.lower()]
                        if error_lines:
                            error_msg += f" (è©³ç´°: {error_lines[0][:200]})"
                    elif 'error' in error_output.lower() or 'not found' in error_output.lower() or 'traceback' in error_output.lower():
                        # æå–é—œéµéŒ¯èª¤ä¿¡æ¯
                        error_lines = [line for line in error_output.split('\n') if 'error' in line.lower() or 'not found' in line.lower()]
                        if error_lines:
                            error_msg = error_lines[0][:300]  # å–ç¬¬ä¸€è¡ŒéŒ¯èª¤ä¿¡æ¯
                        else:
                            error_msg = error_output[:300]
                    else:
                        error_msg = error_output[:300]
                    return {'success': False, 'error': error_msg}
                
                return {
                    'success': True, 
                    'output_size': 0, 
                    'md_count': 0, 
                    'output_dir': str(workspace_dir),
                    'warning': 'å‘½ä»¤åŸ·è¡ŒæˆåŠŸä½†æœªæ‰¾åˆ°è¼¸å‡ºæ–‡ä»¶'
                }
        else:
            # å‘½ä»¤å¤±æ•—ï¼Œçµ„åˆéŒ¯èª¤ä¿¡æ¯
            if not error_output:
                error_msg = f"å‘½ä»¤åŸ·è¡Œå¤±æ•—ï¼Œè¿”å›ç¢¼: {result.returncode}"
            else:
                # æª¢æŸ¥å„ç¨®éŒ¯èª¤é¡å‹
                if has_compatibility_error:
                    error_msg = "vLLM å…§éƒ¨éŒ¯èª¤ï¼Œå¯èƒ½æ˜¯æ¨¡å‹è¼‰å…¥æˆ–é…ç½®å•é¡Œ"
                    error_lines = [line for line in error_output.split('\n') if 'attributeerror' in line.lower() and '_inductor' in line.lower()]
                    if error_lines:
                        error_msg += f" (è©³ç´°: {error_lines[0][:200]})"
                elif has_gpu_memory_error:
                    error_msg = "GPU è¨˜æ†¶é«”ä¸è¶³ã€‚å»ºè­°é™ä½ --gpu-memory-utilization æˆ– --max_model_len åƒæ•¸"
                    # æå–è©³ç´°éŒ¯èª¤ä¿¡æ¯
                    error_lines = [line for line in error_output.split('\n') if 'gpu memory' in line.lower() or 'kv cache' in line.lower() or 'memory' in line.lower()]
                    if error_lines:
                        error_msg = error_lines[0][:300]
                elif has_vllm_server_error:
                    error_msg = "vLLM æœå‹™å™¨å•Ÿå‹•å¤±æ•—ã€‚å¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ"
                    error_lines = [line for line in error_output.split('\n') if 'vllm server' in line.lower()]
                    if error_lines:
                        error_msg += f" (è©³ç´°: {error_lines[0][:200]})"
                else:
                    # æå–é—œéµéŒ¯èª¤ä¿¡æ¯
                    error_lines = [line for line in error_output.split('\n') if 'error' in line.lower() or 'not found' in line.lower() or 'traceback' in line.lower()]
                    if error_lines:
                        error_msg = error_lines[0][:300]  # å–ç¬¬ä¸€è¡ŒéŒ¯èª¤ä¿¡æ¯
                    else:
                        error_msg = error_output[:300]
            return {'success': False, 'error': error_msg}

    except FileNotFoundError:
        return {'success': False, 'error': 'olmOCR æ¨¡çµ„æœªæ‰¾åˆ°ï¼Œè«‹ç¢ºèªå·²å®‰è£ olmocr'}
    except subprocess.TimeoutExpired:
        return {'success': False, 'error': 'è™•ç†è¶…æ™‚ï¼ˆè¶…é900ç§’ï¼‰'}
    except Exception as e:
        return {'success': False, 'error': str(e)}

def analyze_results(results):
    """åˆ†æè™•ç†çµæœ"""
    if not results:
        return

    total_files = len(results)
    success_count = sum(1 for r in results if r['success'])
    total_size = sum(r['size_mb'] for r in results)
    total_time = sum(r['process_time'] for r in results)

    print(f"\nğŸ“Š è™•ç†çµæœ:")
    print(f"æˆåŠŸç‡: {success_count}/{total_files} ({success_count/total_files*100:.1f}%)")
    print(f"ç¸½å¤§å°: {total_size:.1f}MB")
    print(f"ç¸½æ™‚é–“: {total_time:.2f}ç§’")
    if total_time > 0:
        print(f"å¹³å‡é€Ÿåº¦: {total_size/total_time:.2f}MB/ç§’")

    # é¡¯ç¤ºéŒ¯èª¤
    for r in results:
        if not r['success']:
            print(f"âŒ {r['file']}: {r['error']}")

    # ä¿å­˜çµæœ
    output_file = Path(__file__).parent / 'output' / 'olmocr_results.json'
    output_file.parent.mkdir(exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"çµæœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """åŸ·è¡ŒPDFæ‰¹é‡è™•ç†"""
    print("ğŸ”¬ olmOCR PDFè™•ç†")
    print("=" * 50)
    
    # é¡¯ç¤º GPU é…ç½®
    if GPU_DEVICE is not None:
        print(f"âš™ï¸  é…ç½®ï¼šä½¿ç”¨ GPU {GPU_DEVICE}")
    else:
        print(f"âš™ï¸  é…ç½®ï¼šä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU")
    print()
    
    # æª¢æŸ¥GPUç‹€æ…‹
    print("1ï¸âƒ£ GPUè¨˜æ†¶é«”æª¢æŸ¥...")
    check_gpu_memory()
    
    # æª¢æŸ¥ olmOCR
    print("\n2ï¸âƒ£ æª¢æŸ¥ olmOCR ä¾è³´...")
    olmocr_ok = check_olmocr()
    if not olmocr_ok:
        print("\nâŒ olmOCR æœªå®‰è£ï¼Œç„¡æ³•ç¹¼çºŒè™•ç†")
        print("è«‹å…ˆå®‰è£ olmOCR å¾Œå†é‹è¡Œ demo")
        return
    
    # æª¢æŸ¥ vLLM
    vllm_ok = check_vllm()
    if not vllm_ok:
        print("âš ï¸  vLLM æœªå®‰è£ï¼Œå¯èƒ½æœƒå°è‡´è™•ç†å¤±æ•—")
        print("   è«‹ç¢ºèªå·²æ­£ç¢ºå®‰è£ olmocr[gpu]")
    
    # é ä¸‹è¼‰æ¨¡å‹ (å¯é¸)
    print(f"\n3ï¸âƒ£ é å‚™éšæ®µ...")
    pre_download_model()
    
    # è™•ç†PDFæ–‡ä»¶
    print(f"\n4ï¸âƒ£ é–‹å§‹è™•ç†...")
    results = process_pdfs()
    
    # åˆ†æçµæœ
    if results:
        analyze_results(results)
    else:
        print("\nâŒ æ²’æœ‰è™•ç†ä»»ä½•æ–‡ä»¶")
        print("\nğŸ”§ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
        print("1. æª¢æŸ¥test_pdfsç›®éŒ„æ˜¯å¦å­˜åœ¨")
        print("2. ç¢ºèªç›®éŒ„ä¸­æœ‰PDFæ–‡ä»¶")
        print("3. æª¢æŸ¥æ–‡ä»¶æ¬Šé™")

if __name__ == "__main__":
    main()

